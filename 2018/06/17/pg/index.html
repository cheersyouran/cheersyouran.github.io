<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><meta name="google-site-verification" content="9rI-R9CaRiMYlQ9aB6hFKjKp9cwmAm3TeV2szbNzuoI"><meta name="baidu-site-verification" content="qmVEp0OVpw"><title>强化学习——从随机策略梯度到确定性策略梯度 | 西部世界</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">强化学习——从随机策略梯度到确定性策略梯度</h1><a id="logo" href="/.">西部世界</a><p class="description">西蒙的个人博客</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">强化学习——从随机策略梯度到确定性策略梯度</h1><div class="post-meta">Jun 17, 2018<span> | </span><span class="category"><a href="/categories/技术/">技术</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3,807</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 17</span><span class="post-meta-item-text"> 分钟</span></span></span></div><a class="disqus-comment-count" href="/2018/06/17/pg/#vcomment"><span class="valine-comment-count" data-xid="/2018/06/17/pg/"></span><span> 条评论</span></a><div class="post-content"><p><em>本文涉及算法的实现请参照我的<a href="https://github.com/cheersyouran/reinforcement-learning" target="_blank" rel="noopener">Github</a></em></p>
<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p>按照<strong>有无模型</strong>分类，强化学习可以分为Model-Based、Model-Free两类，其中Model-Free又可以分为Prediction、Control两类。<br>按照<strong>优化目标</strong>来分类，强化学习可以分为Value-Based、Policy-Based、Actor-Critic三类。</p>
<p>本文主要说一下Policy-Based方法中的<strong>Stochastic Policy</strong>、<strong>Deterministic policy</strong>以及<strong>Actor-Critic</strong>方法。</p>
<h2 id="2-Policy-Based方法"><a href="#2-Policy-Based方法" class="headerlink" title="2. Policy-Based方法"></a>2. Policy-Based方法</h2><p>Value-Based方法是通过DP、TD或MC来更新$Q(s,a)$值，并基于$Q(s,a)$值采用 $greedy$或$\epsilon-greedy$策略选择下一个动作。所以我们的策略是根据$Q(s,a)$值<strong>间接</strong>得到的。</p>
<p>Policy-Based方法不需要学习$Q(s,a)$值，而是根据DP、TD或MC的回报值$R_t$去直接更新策略。所以我们策略是根据输入的状态s<strong>直接</strong>得到的。</p>
<p>Policy Based方法具体分为两种：Stochastic policy 和 Deterministic policy。区别是前者输出的是动作的概率，后者输出的是一个确定的动作。Policy-Based和Value-Based的优缺点如下：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>适用场景</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>Value-Based</td>
<td>离散有限的动作空间</td>
<td>可以每步更新，算法效率较高；可以收敛到全局最优</td>
<td>无法处理高维连续动作空间</td>
</tr>
<tr>
<td>Policy-Based</td>
<td>高维连续的动作空间</td>
<td>更好的收敛性；随机策略自带探索性质</td>
<td>每个episode更新一次，效率较低，方差较大；容易收敛到局部最优；</td>
</tr>
</tbody>
</table>
<h3 id="2-1-随机策略-Stochastic-Policy"><a href="#2-1-随机策略-Stochastic-Policy" class="headerlink" title="2.1 随机策略(Stochastic Policy)"></a>2.1 随机策略(Stochastic Policy)</h3><p>随机策略常用如下公式表示：$$\pi_\theta(s,a) = P[a|s,\theta]$$其意义是：在状态s时，动作a符合参数为$\theta$的概率分布。我们的目标是找到最佳策略$\pi_\theta(s,a)$对应的参数$\theta$，因此我们必须有效衡量一个策略的好坏。我们把这个衡量方式称为Performance Objective $J(\theta)$，在神经网络中又称损失函数（目标函数）。常用的$J(\theta)$有如下三种：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>start value</td>
<td>$J_1(\theta)=V^{\pi_\theta}(s_1)=E_{\pi_\theta}[V_1]$</td>
</tr>
<tr>
<td>average value</td>
<td>$J_{avV}(\theta)=\sum_sd^{\pi_\theta}(s)V^{\pi_\theta}(s)$</td>
</tr>
<tr>
<td>average reward per time-step</td>
<td>$J_{avR}(\theta)=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)R_s^a$</td>
</tr>
</tbody>
</table>
<p>有了$J(\theta)$，我们就可以设计一个神经网络来最大化$J(\theta)$（或最小化$-J(\theta)$）。总结来说，在这个神经网络中，输入是当前agent获得的状态s，输出是策略$\pi_\theta(s,a)$，目标是最大化损失函数$J(\theta)$，参数$\theta$的更新方式是梯度上升。</p>
<p>那随机策略$\pi_\theta(s,a)$究竟是什么呢，又如何表示呢？在介绍策略之前，先介绍一下策略的梯度：<br>$$<br>\begin{equation}\begin{split}<br>\nabla_\theta\pi_\theta(s,a)&amp;=\pi_\theta(s,a)\frac{\nabla_\theta\pi_\theta(s,a)}{\pi_\theta(s,a)}\\<br>&amp;=\pi_\theta(s,a)\nabla_\theta log\pi_\theta(s,a)<br>\end{split}\end{equation}<br>$$推导很简单，其中$\nabla_\theta log\pi_\theta(s,a)$又被称为score function。我们采用$J_{avR}(\theta)$，则$\nabla_ \theta J(\theta)$为:$$\begin{equation}\begin{split}<br>\nabla_ \theta J(\theta)&amp;=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)\nabla_\theta log\pi_\theta(s,a)R_s^a\\<br>&amp;=E_{s \sim d^\pi, a \sim \pi_\theta}[\nabla_\theta log\pi_\theta(s,a)r]\\<br>&amp;=E_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)r]<br>\end{split}\end{equation}<br>$$通过公式我们可以发现，$\nabla_ \theta J(\theta)$只跟score function和reward有关。<br>回到策略，常用的随机策略有以下两种：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>公式</th>
<th>score function</th>
<th>性质</th>
</tr>
</thead>
<tbody>
<tr>
<td>Softmax策略</td>
<td>$\pi_\theta(s,a) \propto e^{\phi(s,a)^T\theta}$</td>
<td>$\phi(s,a)-E_{\pi_\theta}[\phi(s,·)]$</td>
<td>适用于离散动作空间</td>
</tr>
<tr>
<td>Gaussian策略</td>
<td>$a \sim N(\mu,\sigma^2), \mu=\phi(s)^T\theta$</td>
<td>$\frac{(a-\mu)\phi(s)}{\sigma^2}$</td>
<td>适用于连续动作空间</td>
</tr>
</tbody>
</table>
<p>Softmax策略需要将动作离散化，然后按照softmax之后的Q(s,a)值来随机选择动作。Gaussian策略最终会生成一个高斯分布函数，然后按照该分布随机选择动作。如果使用神经网络实现，Softmax策略只需要在网络的最后一层加入一个softmax函数，然后按照softmax之后的概率值选择动作；Gaussian策略一般默认$\sigma$是一个常量，然后用网络来回归$\mu=\phi(s)^T\theta$中的参数$\theta$，然后基于正态分布$N(\mu,\sigma^2)$来随机动作。</p>
<p>到这里，随机策略梯度的原理基本清晰了。但有几点值得注意：</p>
<ol>
<li>Policy based方法中，我们只能得到策略梯度$\nabla J(\theta)$的公式，却并不存在关于一个损失函数的公式。<strong>策略梯度定理</strong>通过严格的证明(参见<a href="https://www.kth.se/social/files/58d506eff27654042836ace7/AllPolicyGradientLecture.pdf" target="_blank" rel="noopener">证明</a>)，得到了与[环境状态转移(transition probability)]无关的$\nabla J(\theta)$。网上普遍说的损失函数$Loss = log\pi_\theta(s,a) r$只是根据$\nabla J(\theta)$反推出来的或根据交叉熵理论推演出来，尚未找到理论证明。在tensorflow中，可以不用tf.minimize而是直接用tf.compute_gradient和tf.apply_gradient来跳过损失函数的问题，当然，使用这种交叉熵$\nabla J(\theta)$也可以同样的结果。</li>
<li>$Loss = log\pi_\theta(s,a) r$函数中的r需要精心设计。应设计避免 r恒&gt;0或 r恒&lt;0，否者容易导致更新方向固定（恒增或恒减），导致过快收敛于局部最优。理想的r是$E[r]=0$，可参见<a href="https://github.com/cheersyouran/reinforcement-learning" target="_blank" rel="noopener">Github</a>中的例子。若无法避免，应降低学习率，防止过快收敛于局部最优。这也符合Advantages函数的思想。</li>
<li><p>policy based方法必须等episode结束才能获得reward，才能更新。这种每回合更新一次的算法叫Monte-Carlo Policy Gradient，又叫REINFORCE。参数的更新公式如下：$$\Delta\theta_t=\alpha\nabla_\theta log\pi_\theta(s_t, a_t)v_t$$$$v_t=R_{t+1} + \gamma R_{t+2} + …+\gamma^{T-1}R_{t+2}$$完整的REINFORCE算法如下：<img src="/images/reinforce.png" width="500" alt="REINFROCE algorithm" align="center"></p>
</li>
<li><p>严格的来讲，上述PG算法属于on-policy PG，即目标策略和采样策略是相同的策略。off-policy PG公式如下：$$\begin{equation}\begin{split}<br>\nabla_\theta J_\beta(\pi_\theta) &amp;= E_{s \sim \rho^\beta, a \sim \pi_\theta}[\nabla_\theta log \pi_\theta(s,a) Q^\pi(s,a)]\\<br>&amp; = E_{s \sim \rho^\beta, a \sim \beta}[\frac{\pi_\theta(s,a)}{\beta_\theta (s,a)} \nabla_\theta log \pi_\theta(s,a) Q^\pi(s,a)]<br>\end{split}\end{equation}$$其中，$\frac{\pi_\theta(s,a)}{\beta_\theta (s,a)}$是重要性采样。</p>
</li>
</ol>
<h3 id="2-2-Actor-Critic算法"><a href="#2-2-Actor-Critic算法" class="headerlink" title="2.2 Actor-Critic算法"></a>2.2 Actor-Critic算法</h3><h3 id="2-2-1-QAC"><a href="#2-2-1-QAC" class="headerlink" title="2.2.1 QAC"></a>2.2.1 QAC</h3><p>在2.1所描述的REINFROCE算法中，$v_t$虽然是$Q^{\pi_\theta}(s,a)$的抽样无偏估计(unbiased)，但是由于MC抽样的随机性，具有较高的方差(high variance)。如果有可以用一个函数来估计价值函数 $Q_w(s,a) \approx Q^{\pi_\theta}(s,a)$，此时$$\nabla_ \theta J(\theta) \approx E_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_w(s,a)]$$那么不仅可以减少方差，还可以达到<strong>每步更新</strong>，大大提高了算法的效率。这个算法的确有，叫Actor-Critic。</p>
<p>Actor-Critic算法由Actor和Critic两个模块构成：</p>
<table>
<thead>
<tr>
<th>模块名称</th>
<th>作用</th>
<th>更新方式</th>
<th>更新公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>Actor</td>
<td>选择动作</td>
<td>update $\theta$ by Policy Gradient Ascent</td>
<td>$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t, a_t)Q_w(s,a)$</td>
</tr>
<tr>
<td>Critic</td>
<td>估计$Q^{\pi_\theta}(s,a)$</td>
<td>update $w$ by TD(0)</td>
<td>$w = w + \beta(r + \gamma Q_w(s’,a’) - Q_w(s,a))\nabla_\theta Q_w(s,a)$</td>
</tr>
</tbody>
</table>
<p>完整的Actor-Critic算法如下：<br><img src="/images/ac.png" width="500" alt="ac algorithm" align="center"></p>
<p>注意：QAC算法中，仅使用一个线性价值函数$Q_w(s,a)=\phi(s,a)^Tw$来逼近状态行为价值函数$Q^{\pi_\theta}(s,a)$，而没用非线性的神经网络。</p>
<h3 id="2-2-2-Compatible-Function-Approximation"><a href="#2-2-2-Compatible-Function-Approximation" class="headerlink" title="2.2.2 Compatible Function Approximation"></a>2.2.2 Compatible Function Approximation</h3><p>在2.2.1中，我们得到了$Q^{\pi_\theta}(s,a)$的近似表示$Q_w(s,a)=\phi(s,a)^Tw$，但很遗憾，$Q_w(s,a)$始终是有偏估计，一个有偏的Q值下得到的策略梯度不一是最好的。比如近似值函数$Q_w(s,a)$可能会引起状态重名等。<br>当$Q_w(s,a)$满足如下两个条件时:</p>
<ol>
<li>$ \nabla_w Q_w(s,a)=\nabla_\theta log\pi_\theta(s,a)$</li>
<li>参数使误差$\varepsilon$最小化：$ \varepsilon = E_{\pi_\theta}[(Q^{\pi_\theta}(s,a) - Q_w(s,a))^2]$</li>
</ol>
<p>那么我们就可以得到：$$Q_w(s,a) = Q^{\pi_\theta}(s,a)$$$$\nabla_ \theta J(\theta) = E_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_w(s,a)]$$过程很容易证明，只需要令(2)式$\nabla_w \varepsilon$=0即可。略。</p>
<h3 id="2-2-3-Advantages-Actor-Critic"><a href="#2-2-3-Advantages-Actor-Critic" class="headerlink" title="2.2.3 Advantages Actor-Critic"></a>2.2.3 Advantages Actor-Critic</h3><p>在2.2.2中，我们得到了$Q^{\pi_\theta}(s,a) = Q_w(s,a)$的条件。再此基础上，我们还可以继续改进AC算法。考虑到不同的$Q^{\pi\theta}(s,a)$差距非常大，如上一次100下一次-100等，这样会造成很大的方差。<br>引入Baseline $B(s)$可以一定程度上消除方差。$B(s)$是一个仅与状态值函数$V(s)$有关，跟动作值函数$Q(s,a)$无关的函数。并有如下性质：$$\begin{equation}\begin{split}<br>E_{\pi_\theta}[\nabla_\theta log_{\pi\theta}(s,a)B(s)]&amp;=\sum_s d^{\pi_\theta}(s) \sum_a \nabla_\theta \pi_\theta(s,a)B(s)\\<br>&amp;=\sum_s d^{\pi_\theta}(s)B(s) \nabla_\theta \sum_a \pi_\theta(s,a)\\<br>&amp;=0<br>\end{split}\end{equation}$$故可知：$$E_{\pi_\theta}[\nabla_\theta log_{\pi\theta}(s,a)Q^{\pi_\theta}(s,a)]=<br>E_{\pi_\theta}[\nabla_\theta log_{\pi\theta}(s,a)(Q^{\pi_\theta}(s,a) \pm B(s))]$$</p>
<p>B(s)可以有很多种选择方式，比较常见的是选择$B(s) = V^{\pi_\theta}(s)$，此时可以定义adavantages function $A^{\pi_\theta}(s,a)$为：$$A^{\pi_\theta}(s,a)=Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s)$$同时$\nabla_ \theta J(\theta)$重新表示为：$$\nabla_ \theta J(\theta) = E_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)A^{\pi_\theta}(s,a)]<br>$$势函数$A^{\pi_\theta}(s,a)$的含义：在s状态采取行为a时，结果相对于状态s的平均值的改善程度。由于减去了状态s的平均估值，故而有效的减小了$Q^{\pi\theta}(s,a)$的方差，使得训练过程更平稳。</p>
<p>现在我们有了2个价值模型$V^{\pi_\theta}(s)$和$Q^{\pi_\theta}(s,a)$，故需要2个函数$V_v(s)和Q_w(s,a)$去逼近。<br>$$<br>\begin{equation}\begin{split}<br>V_v(s)&amp;\approx V^{\pi_\theta}(s)\\<br>Q_w(s,a) &amp;\approx Q^{\pi_\theta}(s,a)\\<br>A(s,a) &amp;= Q_w(s,a) - V_v(s)\\<br>\end{split}\end{equation}<br>$$</p>
<p>现在，我们用两个函数已经可以表示$A^{\pi_\theta}(s,a)$，但这样做显然很复杂。有一种很好地解决方法：<br>$$\begin{equation}\begin{split}<br>\delta^{\pi_\theta}&amp;=r+\gamma V^{\pi_\theta}(s’) - V^{\pi_\theta}(s)\\<br>E_{\pi_\theta}[\delta^{\pi_\theta}|s,a] &amp;= E_{\pi_\theta}[r+\gamma V^{\pi_\theta}(s’)|(s,a)] - V^{\pi_\theta}(s)\\<br>&amp;=Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s)\\<br>&amp;=A^{\pi_\theta}(s,a)\\<br>所以，\nabla_ \theta J(\theta) &amp;= E_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)\delta^{\pi_\theta}]<br>\end{split}\end{equation}<br>$$实际中，我们用TD的函数逼近形式：$$\delta_v=r+\gamma V_v(s’) - V_v(s)$$这样就只需要更新一个函数的参数v。<br>现在，我们重新整理一下Actor-Critic方法：</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>MC</th>
<th>TD(0)</th>
<th>TD($\lambda$)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Actor</td>
<td>$\Delta \theta = \alpha(v_t - V_v(s_t)) · $ <br>$ \nabla_\theta log \pi_\theta(s_t,a_t)$</td>
<td>$\Delta \theta = \alpha(r + \gamma V_v(s_{t+1}) - V_v(s_t)) · $ <br>$ \nabla_\theta log \pi_\theta(s_t,a_t)$</td>
<td>$\Delta \theta = \alpha(v^\lambda_t - V_v(s_t)) · $ <br>$ \nabla_\theta log \pi_\theta(s_t,a_t)$</td>
</tr>
<tr>
<td>Critic</td>
<td>$\Delta \theta = \alpha(v_t - V_\theta(s))\phi(s)$</td>
<td>$\Delta \theta = \alpha(r + \gamma V(s’) - V_\theta(s))\phi(s)$</td>
<td>$\Delta \theta = \alpha(v^\lambda_t - V_\theta(s))\phi(s)$</td>
</tr>
</tbody>
</table>
<p>随机策略梯度的多种形式的总结：<img src="/images/pg-summary.png" width="500" alt="REINFROCE algorithm" align="center"></p>
<h3 id="2-3-确定性策略-Deterministic-policy"><a href="#2-3-确定性策略-Deterministic-policy" class="headerlink" title="2.3 确定性策略(Deterministic policy)"></a>2.3 确定性策略(Deterministic policy)</h3><p>之前说的Stochastic Policy、Actor-Critic都是基于随机策略的。确定性策略跟随机策略不同，确定性策略在同一个状态s的动作a是唯一确定的。确定性策略的公式为：$$a = \mu_\theta(s)$$简单比较一下确定性策略和随机策略：</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>确定性策略</td>
<td>$\nabla_\theta J(\mu_\theta)$只对状态积分，故需要采样的数据少，效率高</td>
<td>容易局部最优</td>
</tr>
<tr>
<td>随机策略</td>
<td>将探索和改进集成到一个策略中</td>
<td>$\nabla_\theta J(\pi_\theta)$对状态和动作一起积分，算法效率低，需要大量训练数据</td>
</tr>
</tbody>
</table>
<p>确定性策略的Performance Objective：$$\begin{equation}\begin{split}<br>J(\mu_\theta) &amp;= \int_S \rho^u(s)r(s,u_\theta(s))ds\\<br>&amp;=E_{s \sim \rho^\mu}[r(s,u_\theta(s))]\\<br>&amp;=E_{s \sim \rho^\mu, a = u_\theta(s)}[r(s,a)]\\<br>&amp;=E_{\mu_\theta}[r(s,a)]\\<br>\end{split}\end{equation}$$确定性策略梯度：<br>$$\begin{equation}\begin{split}<br>\nabla_\theta J(\mu_\theta) =\int_S \rho^u(s)\nabla_\theta\mu_\theta(s)\nabla_a Q^{\mu}(s,a)|_{a=\mu\theta(s)}ds\\<br>\end{split}\end{equation}<br>$$</p>
<p>$$=E_{s \sim \rho^\mu, a = u_\theta(s)}[\nabla_\theta\mu_\theta(s)\nabla_a Q^{\mu}(s,a)]$$</p>
<p>实际上，Silver在论文中已证明，确定性策略是随机策略在方差为0时的一种特殊形式。</p>
<h3 id="2-4-Determistic-Actor-Critic"><a href="#2-4-Determistic-Actor-Critic" class="headerlink" title="2.4 Determistic Actor-Critic"></a>2.4 Determistic Actor-Critic</h3><p>在2.3节中，我们将随机行为策略（Actor）与价值策略（Critic）结合，引入了Stochastic Actor-Critic算法。同样的，我们也可以将确定性行为策略（Actor）与价值策略（Critic）结合构造一个Determistic Actor-Critic算法，又叫Determistic Policy Gradient算法。</p>
<p>与QAC算法相似的，在DPG中引入一个近似函数$Q^w(s,a)$来估计$Q^{\mu_\theta}$，$Q^w(s,a) \approx Q^{\mu_\theta}(s,a)$。此时我们得到$$\nabla_\theta J(\mu_\theta) \approx E_{s \sim \rho^\mu, a = u_\theta(s)}[\nabla_\theta\mu_\theta(s)\nabla_a Q^w(s,a)]$$</p>
<p>DPG算法的更新方法如下：<br>$$\begin{equation}\begin{split}<br>\delta_t &amp;= r_t+ \gamma Q^w(s_{t+1}, \mu_theta(s_{t+1})) - Q^w(s_t, a_t)\\<br>w_{t+1} &amp;= w_t + \alpha_w \delta_t \nabla_w Q^w(s_t, a_t)\\<br>\theta_{t+1} &amp;= \theta_t + \alpha_\theta \nabla_\theta \mu_\theta(s_t) \nabla_\alpha Q^w(s_t, a_t)<br>\end{split}\end{equation}<br>$$</p>
<p>值得注意的是，DPG又细分为on-policy和off-policy。上述DPG算法是on-policy的，即想要学习的策略和用于采样的策略相同，critic用sarsa算法训练。off-policy中想要学习的策略和用于采样的策略不同，critic用q-learning训练。不同的采样策略会导致状态s服从不同的分布，所以策略梯度公式会稍稍有变化，参数的更新方式不变：$$\nabla_\theta J(\mu_\theta) \approx E_{s \sim \rho^\beta, a = u_\theta(s)}[\nabla_\theta\mu_\theta(s)\nabla_a Q^w(s,a)]$$</p>
<p>此外，当满足一下CFA条件时，$Q^w(s,a) = Q^{\mu_\theta}(s,a)$:</p>
<ol>
<li>$ \nabla_\theta \mu_\theta(s)^Tw = \nabla_a Q^w(s,a)|_{a=\mu\theta(s)}$</li>
<li>参数使误差$\varepsilon$最小化：$ \varepsilon = E_{\mu_\theta}[(Q^w(s,a)- Q^{\mu_\theta}(s,a))^2]$</li>
</ol>
<p>满足上述条件的$Q^w(s,a)$的线性拟合有很多，比如$Q^w(s,a) = A^w(s,a) + V^v(s)$, 其中$A^w(s,a) = \phi(s,a)^Tw = [\nabla_\theta \mu_\theta(s)(a - \mu_\theta(s))]^Tw$（这里我也不知道为什么要设计成$A^w(s,a) + V^v(s)$）。</p>
<h3 id="2-5-Deep-Determistic-Policy-Gradient"><a href="#2-5-Deep-Determistic-Policy-Gradient" class="headerlink" title="2.5 Deep Determistic Policy Gradient"></a>2.5 Deep Determistic Policy Gradient</h3><p>DDPG是将深度学习神经网络融合进off-policy DPG的策略学习方法。DDPG针对DPG做了如下改进：</p>
<ol>
<li>DPG使用线性函数通过线性回归拟合$Q^{\mu_\theta}$，而DDPG使用神经网络拟合$Q^{\mu_\theta}$。</li>
<li>借鉴DQN的成功经验，采用经验回放（experience replay）的方式做离线训练，打破数据的时序关联性，保证算法的收敛。</li>
<li>借鉴DQN的成功经验，引入了online和target两种网络。并采用soft的方式从online critic和online actor对target critic和target actor两个网络进行更新，使学习过程更加稳定。</li>
<li>采用Batch Normalization解决feature量纲不同的问题</li>
</ol>
<p>DDPG算法的伪代码：<img src="/images/ddpg.png" width="550" alt="ddpg algorithm" align="center"></p>
<p><em>本文涉及算法的实现请参照我的<a href="https://github.com/cheersyouran/reinforcement-learning" target="_blank" rel="noopener">Github</a></em></p>
<h2 id="3-参考"><a href="#3-参考" class="headerlink" title="3. 参考"></a>3. 参考</h2><p>[1] David Silver. et al. 2014. Deterministic policy gradient algorithms. ICML’14, Vol. 32. JMLR.org I-387-I-395.<br>[2] Richard S. Sutton. et al. 1999. Policy gradient methods for reinforcement learning with function approximation. NIPS’99, MIT Press, Cambridge, MA, USA, 1057-1063.<br>[3] Lillicrap, Timothy P. et al. “Continuous control with deep reinforcement learning.” CoRR abs/1509.02971 (2015): n. pag.</p>
</div><iframe src="/donate/?AliPayQR=/img/AliPayQR.png&amp;WeChatQR=/img/WeChatQR.png&amp;GitHub=https://github.com/cheersyouran&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>ZHANG Youran</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2018/06/17/pg/">https://cheersyouran.github.io/2018/06/17/pg/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>版权归作者所有，转载请注明出处。</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://cheersyouran.github.io/2018/06/17/pg/" data-id="cjz3uecxq0005o5vr172oiu93" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACIUlEQVR42u3aO47DMAwFwNz/0t42wMLxoygXkkZV4DiOxgUhfj6feF1f6+76//V9z90zk3+ZtjAwMJZl5FvJGXfbTai/77+lYmBgHMD4HRA7gGoYHQvWGBgYGGMBsRqCMTAwMOYG3E52maSyGBgYGHNLZknhrBqmX8zFMTAwFmR0Esu3P7/S38DAwFiKcRVXp9CfbO4aWhgYGHsz8gCXH8vGYNVjHwYGBkY1XI7dn4+FRYdFDAyMAxhjIbWa+o6NnUUvCwMDY2tGtVmYH/L6EbLakMDAwNiPMfeI1im35fc/VA0xMDA2ZeSJZeHRP4P1rJGOwjvAwMBYnJEnsXmSmd/TGfOKOrEYGBhbMPrF+qSI1k9Wb3eLgYGxNaNaROuE4Oqh8JXTLgYGxhaMPGgmmOpxszVsgYGBsSkjCamdYlynAFdonWJgYBzDmPbQ4Dlj4NsQjIGBcSQjTyyrI2JzwQ8BFwMDYwtGtfieX+mMYuTTFIM9BAwMjGUZeXmr07ysht1CGQ4DA+MYRufPxtLRTtsSAwPjNEY/1cyPjA8DE8XmBAYGxt6Mq7jGBsKqMxJ50ouBgXECoxPsohLYpEGxWSEbAwNjXUZ+1KseKHNS/u2n+p4wMDA2YuSBL4fl2+qX5DAwMDCqpbR+mK5ex8DAwOgH3LxJmTzhNonFwMDYmpG3JDutyuqIxivlNgwMjAUZ1dQxJyUbTZLYsV9hYGBswfgDlscZBkgln7wAAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/强化学习/">强化学习</a><a href="/tags/机器学习/">机器学习</a><a href="/tags/策略梯度/">策略梯度</a></div><div class="post-nav"><a class="pre" href="/2018/06/19/alphago/">Alphago的原理</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'bniJ3zoUdRvlPtkBwpdn3cgB-gzGzoHsz',
  appKey:'39V42UT82heESUV7ROEM2YH0',
  placeholder:'请在此输入留言，一起交流进步~',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/技术/">技术</a><span class="category-list-count">5</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Alphago/" style="font-size: 15px;">Alphago</a> <a href="/tags/强化学习/" style="font-size: 15px;">强化学习</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/凸优化/" style="font-size: 15px;">凸优化</a> <a href="/tags/策略梯度/" style="font-size: 15px;">策略梯度</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/08/28/trpo/">TRPO</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/23/trust-reg/">置信域算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/16/cons-ops/">约束优化</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/19/alphago/">Alphago的原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/17/pg/">强化学习——从随机策略梯度到确定性策略梯度</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">西部世界.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>
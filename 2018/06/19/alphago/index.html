<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><meta name="google-site-verification" content="9rI-R9CaRiMYlQ9aB6hFKjKp9cwmAm3TeV2szbNzuoI"><meta name="baidu-site-verification" content="qmVEp0OVpw"><title>Alphago的原理 | 西部世界</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Alphago的原理</h1><a id="logo" href="/.">西部世界</a><p class="description">西蒙的个人博客</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Alphago的原理</h1><div class="post-meta">Jun 19, 2018<span> | </span><span class="category"><a href="/categories/技术/">技术</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1,524</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 6</span><span class="post-meta-item-text"> 分钟</span></span></span></div><a class="disqus-comment-count" href="/2018/06/19/alphago/#vcomment"><span class="valine-comment-count" data-xid="/2018/06/19/alphago/"></span><span> 条评论</span></a><div class="post-content"><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><p>如今AlphaGo的名字已是家喻户晓，它是人工智能领域的非常重要的里程碑事件。我们知道，围棋的特征空间高达361！个，以至于目前的计算力在有限的时间内根本无法完成搜索。本文从纯技术角度聊一聊AlphaGo是如何解决该问题的，探究一下它成功的原因和技术细节。本文大部分细节来自于Deepmind在Nature的论文《Mastering the game of Go with deep neural networks and tree search》。<br>AlphaGo技术点主要分为3大部分：策略网络(Policy Network)、价值网络(Value Network)、蒙特卡洛树搜索(MCTS)。然后采用离线训练+在线博弈的方式，将三种技术结合起来，最终出”棋“致胜。</p>
<h2 id="2-离线训练技术"><a href="#2-离线训练技术" class="headerlink" title="2. 离线训练技术"></a>2. 离线训练技术</h2><p>本节将介绍AlphaGo中的4种离线训练的网络结构，这些训练好的网络将会被应用于围棋的在线博弈中。</p>
<h3 id="2-1-监督学习策略网络-SL-Policy-Network"><a href="#2-1-监督学习策略网络-SL-Policy-Network" class="headerlink" title="2.1 监督学习策略网络(SL Policy Network)"></a>2.1 监督学习策略网络(SL Policy Network)</h3><p>AlphaGo基于3万多人类围棋专家的棋谱，采用监督学习的方式训练了一个13层的Deep Q Network(DQN)，称为SL Policy Network，$p_\sigma$，实现了end-to-end的预测。DQN采用经验回放和target network更新两种方式解决了强化学习中非线性拟合（如神经网络）不收敛的问题。</p>
<p>SL Policy Net的输入是围棋19*19像素图片和一些人工定义的围棋先验知识，输出是当前特征下每个合理动作的概率$p_\sigma(a|s)$，目标最大化似然函数$logp_\sigma (a|s)$，更新方式是梯度上升。</p>
<p>SL Policy的优点是准确率较高，缺点是走子速度相对较慢：基于全部特征的SL Policy准确率达到了57.0%，基于棋盘信息和下棋历史信息的SL Policy准确率达到了55.7%，走一步需要大约3ms。</p>
<h3 id="2-2-快速走子网络-Fast-Rollout-Policy-Network"><a href="#2-2-快速走子网络-Fast-Rollout-Policy-Network" class="headerlink" title="2.2 快速走子网络(Fast Rollout Policy Network)"></a>2.2 快速走子网络(Fast Rollout Policy Network)</h3><p>跟SL Policy Network类似，AlphaGo基于3万多人类围棋专家的棋谱又训练了一个Fast Rollout Policy，$p_\pi$。Fast Rollout Policy与SL Policy的输出完全相同，不同的是Rollout Policy使用了更少的特征和特征的线性组合，牺牲了一定精度换取了更快的走子速度。Rollout Policy的准确率约为24.29%，走子速度约为2$\mu s$。 Rollout Policy主要用于MCTS，快速走子来估计局面。</p>
<h3 id="2-3-强化学习网络-Reinforcement-Learning-Policy-Network"><a href="#2-3-强化学习网络-Reinforcement-Learning-Policy-Network" class="headerlink" title="2.3 强化学习网络(Reinforcement Learning Policy Network)"></a>2.3 强化学习网络(Reinforcement Learning Policy Network)</h3><p>当基于先验知识的SL Policy Network训练好之后，AlphaGo已经拥有人类专家的平均水平了。在此基础上，AlphaGo采用基于策略梯度的强化学习自我对弈的方式做进一步提升，称之为RL Policy Network，$p_\rho$。RL Policy Network的网络结构与SL Policy Network一样，它的参数初始权重$\rho$使用训练好了的SL Policy的参数权重$\sigma$。</p>
<p>对弈过程中，用几回合之前的RL Policy Net和最新RL Policy Net对弈。首先随机行成一个对局的开局，然后从该时刻$t=t_0$开始博弈直到游戏结束$t=t_T$。设置胜方$reward(t_T) = 1$，败方$reward(t_T) = -1$，然后对胜负棋局分别用最大似然更新每一步：$$\sum_{s_t} logp_\sigma (a_t|s_t)reward(t)$$</p>
<h3 id="2-4-价值网络-Value-Network"><a href="#2-4-价值网络-Value-Network" class="headerlink" title="2.4 价值网络(Value Network)"></a>2.4 价值网络(Value Network)</h3><p>Value Network$V^p(s)$用来给当前局面”打分“，即所谓的大局观。Value Network的结构跟RL Policy Net基本一样，只是最后一层的输出由概率分布变成了单个值。<br>Value Network基于以下步骤自我博弈生成了3000万个对局信息：</p>
<ol>
<li>随机生成一个数字 $U \sim N(1, 450)$;</li>
<li>用SL policy network $a_t \sim p_\sigma(·|s_t)$生成前U−1步局面; </li>
<li>第U步随机选择$a_u ~ unif{1, 361}$，但必须符合围棋规则;</li>
<li>从第U+1步开始，使用RL Policy Net$a_t \sim p_\rho(·|s_t)$博弈直到游戏结束，得到游戏结果$reward(t_T)$；</li>
<li>将$(S_{U+1},reward_{U+1})$加入样本池中，用于后续Value Network训练的训练。</li>
</ol>
<p>注意：第1、2、3步增加了样本的多样性；损失函数是MSE。<br><img src="/images/pn&vn.png" width="300" alt="ac algorithm" align="center"></p>
<h2 id="3-在线博弈"><a href="#3-在线博弈" class="headerlink" title="3. 在线博弈"></a>3. 在线博弈</h2><p>在线博弈主要将MCTS与训练好的网络相结合，基于当前局面做出最优的动作选择。</p>
<h3 id="3-1-蒙特卡洛书搜索-MCTS"><a href="#3-1-蒙特卡洛书搜索-MCTS" class="headerlink" title="3.1 蒙特卡洛书搜索(MCTS)"></a>3.1 蒙特卡洛书搜索(MCTS)</h3><p>MCTS是一种用于博弈游戏寻找最优解决方案的算法。MCTS会基于当前局面做多次模拟博弈，并尝试根据模拟结果预测最优的移动方案。AlphaGo中的MCTS进行一次模拟有以下4步：</p>
<ol>
<li><strong>Selection</strong>：在当前局面，根据UCB1算法选择回报值最高的动作：$$a_t = argmax(Q(s_t,a)+\mu(s_t,a)),\quad \mu \propto \frac{P(s,a)}{1+N(s,a)}$$重复选择L步到达一个叶子节点$s_L$。</li>
<li><strong>Expansion</strong>：利用SL Policy Network，$p_\sigma$的将叶节点$s_L$扩展，并将输出的概率分布作为先验概率$P(s,a)=p_{\sigma}(a|s)$存下来。</li>
<li><strong>Evaluation</strong>：评估叶节点$s_L$。采用价值网络进行估值$v_\theta(s_L)$，并采用Rollout Policy $p_\pi$博弈直到游戏结束，获得回报$z_L$。</li>
<li><strong>Backup</strong>：更新蒙特卡洛树。$$\begin{equation}\begin{split}<br>V(s_L) &amp;= (1 - \lambda)v_\theta(s_L) + \lambda z_L\\<br>N(s,a) &amp;= \sum^n_{i=1}1(s,a,i)\\<br>Q(s,a) &amp;= \frac{1}{N(s,a)}\sum^n_{i=1}1(s,a,i)V(s^i_L)<br>\end{split}\end{equation}$$其中$s^i_L$是第i次模拟的叶子节点；$1(s,a,i)$表示(s,a)在第i次模拟是否被访问。当模拟结束，算法会选择访问最多次的动作作为预测动作。</li>
</ol>
<p>下图是AlphaGo Nature论文中对MCTS的总结：<br><img src="/images/MCTS.png" width="650" alt="ac algorithm" align="center"></p>
<h3 id="4-结果模拟"><a href="#4-结果模拟" class="headerlink" title="4. 结果模拟"></a>4. 结果模拟</h3><p><img src="/images/abcdef.png" width="650" alt="ac algorithm" align="center"></p>
<p>a. 基于当前局面，value network $v_\theta(s’)$对所有的候选位置的评估。<br>b. 仅用value network$v_\theta(s’)$做评估，多次模拟后得到平均Q(s,a)。(即$\lambda = 0$)<br>c. 仅用rollouts network$v_\pi(s,a)$做评估，多次模拟后得到平均Q(s,a)。(即$\lambda = 1$)。<br>d. 根据SL policy network $P_\sigma(a|s)$选择的动作概率（大于0.1%）</p>
</div><iframe src="/donate/?AliPayQR=/img/AliPayQR.png&amp;WeChatQR=/img/WeChatQR.png&amp;GitHub=https://github.com/cheersyouran&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>ZHANG Youran</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2018/06/19/alphago/">https://cheersyouran.github.io/2018/06/19/alphago/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>版权归作者所有，转载请注明出处。</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://cheersyouran.github.io/2018/06/19/alphago/" data-id="cjz3uecxh0000o5vr08t3vmkr" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACJUlEQVR42u3aQW4DIQwF0Nz/0ukFGvJtTysBb1aVmhn8WCBj+/WKn/fy+fTLV/GZvIuBgXE2Iwkx+W91lU+hJ+v+siIGBsYFjE8Lrz+dv7XmVb+AgYGBkTDm7PXBioGBgfEUIw+umghiYGBg9C6xSeh5KS1f6+G7OAYGxoaMvOr+/3//SX8DAwNjK8a7+OQJ36TxWY4KAwPjaMY8sesV3eaDGhgYGHcyeiMX+ThFEm7SQhgliBgYGIcyerfIamMgb3M2D1wMDIxtGetA15jRzgVbkCSgGBgYtzHyxea5WXXk4gsbAwPjaEb1ElstiuXDFoXjdX4Xx8DA2JDRO1irbYD1FyZbg4GBcTZjMiSRlP57W1Mu5GFgYFzM6C3Wu6Y2C229zicGBsZWjDyU3oWzN0bWHLbAwMC4gJG/lrc28xDzMl+5joiBgbE5Yz5TVm0e9JqaXzYaAwPjaEZSOKsW4J4q3uVfw8DAuIExCbSaRD51MY7KbRgYGAcxquWtnJEX4CZHOQYGxj2MSXtyvR15Wb8aQ7kKiIGBsTkjD2hSyu/xogQRAwPjUMa7+EzaCflRW73iYmBgnM3oHXb5mFe16J9vR+/IxsDA2JdRbTFWeZN2QmG/MTAwLmBUD748mXs27fuS52JgYGAEB2618ZmMf2FgYGBMDtwkiN7gV/kajIGBcQEjbwbkA17VxkDy5QfKbRgYGBsyqlfHyYhYnnrmGAwMjKMZPzFiapgSVCRoAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/Alphago/">Alphago</a><a href="/tags/强化学习/">强化学习</a><a href="/tags/机器学习/">机器学习</a></div><div class="post-nav"><a class="pre" href="/2018/08/16/cons-ops/">约束优化</a><a class="next" href="/2018/06/17/pg/">强化学习——从随机策略梯度到确定性策略梯度</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'bniJ3zoUdRvlPtkBwpdn3cgB-gzGzoHsz',
  appKey:'39V42UT82heESUV7ROEM2YH0',
  placeholder:'请在此输入留言，一起交流进步~',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/技术/">技术</a><span class="category-list-count">5</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Alphago/" style="font-size: 15px;">Alphago</a> <a href="/tags/强化学习/" style="font-size: 15px;">强化学习</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/凸优化/" style="font-size: 15px;">凸优化</a> <a href="/tags/策略梯度/" style="font-size: 15px;">策略梯度</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/08/28/trpo/">TRPO</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/23/trust-reg/">置信域算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/16/cons-ops/">约束优化</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/19/alphago/">Alphago的原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/17/pg/">强化学习——从随机策略梯度到确定性策略梯度</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">西部世界.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>
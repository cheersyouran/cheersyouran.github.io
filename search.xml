<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Alphago的原理</title>
      <link href="/2018/06/19/alphago/"/>
      <url>/2018/06/19/alphago/</url>
      
        <content type="html"><![CDATA[<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><p>如今AlphaGo的名字已是家喻户晓，它是人工智能领域的非常重要的里程碑事件。我们知道，围棋的特征空间高达361！个，以至于目前的计算力在有限的时间内根本无法完成搜索。本文从纯技术角度聊一聊AlphaGo是如何解决该问题的，探究一下它成功的原因和技术细节。大部分细节来自于Deepmind在Nature的论文《Mastering the game of Go with deep neural networks and tree search》。<br>AlphaGo技术点主要分为3大部分：策略网络(Policy Network)、价值网络(Value Network)、蒙特卡洛树搜索(MCTS)。然后采用离线训练+在线博弈的方式，将三种技术结合起来，最终出”棋“致胜。</p><h2 id="2-离线训练技术"><a href="#2-离线训练技术" class="headerlink" title="2. 离线训练技术"></a>2. 离线训练技术</h2><p>本节将介绍AlphaGo中的4种离线训练的网络结构，这些训练好的网络将会被应用于围棋的在线博弈中。</p><h3 id="2-1-监督学习策略网络-SL-Policy-Network"><a href="#2-1-监督学习策略网络-SL-Policy-Network" class="headerlink" title="2.1 监督学习策略网络(SL Policy Network)"></a>2.1 监督学习策略网络(SL Policy Network)</h3><p>AlphaGo基于3万多人类围棋专家的棋谱，采用监督学习的方式训练了一个13层的Deep Q Network(DQN)，称为SL Policy Network，$p_\sigma$，实现了end-to-end的预测。DQN采用经验回放和target network更新两种方式解决了强化学习中非线性拟合（如神经网络）不收敛的问题。</p><p>SL Policy Net的输入是围棋19*19像素图片和一些人工定义的围棋先验知识，输出是当前特征下每个合理动作的概率$p_\sigma(a|s)$，目标最大化似然函数$logp_\sigma (a|s)$，更新方式是梯度上升。</p><p>SL Policy的优点是准确率较高，缺点是走子速度相对较慢：基于全部特征的SL Policy准确率达到了57.0%，基于棋盘信息和下棋历史信息的SL Policy准确率达到了55.7%，走一步需要大约3ms。</p><h3 id="2-2-快速走子网络-Fast-Rollout-Policy-Network"><a href="#2-2-快速走子网络-Fast-Rollout-Policy-Network" class="headerlink" title="2.2 快速走子网络(Fast Rollout Policy Network)"></a>2.2 快速走子网络(Fast Rollout Policy Network)</h3><p>跟SL Policy Network类似，AlphaGo基于3万多人类围棋专家的棋谱又训练了一个Fast Rollout Policy，$p_\pi$。Fast Rollout Policy与SL Policy的输出完全相同，不同的是Rollout Policy使用了更少的特征和特征的线性组合，牺牲了一定精度换取了更快的走子速度。Rollout Policy的准确率约为24.29%，走子速度约为2$\mu s$。 Rollout Policy主要用于MCTS，快速走子来估计局面。</p><h3 id="2-3-强化学习网络-Reinforcement-Learning-Policy-Network"><a href="#2-3-强化学习网络-Reinforcement-Learning-Policy-Network" class="headerlink" title="2.3 强化学习网络(Reinforcement Learning Policy Network)"></a>2.3 强化学习网络(Reinforcement Learning Policy Network)</h3><p>当基于先验知识的SL Policy Network训练好之后，AlphaGo已经拥有人类专家的平均水平了。在此基础上，AlphaGo采用基于策略梯度的强化学习自我对弈的方式做进一步提升，称之为RL Policy Network，$p_\rho$。RL Policy Network的网络结构与SL Policy Network一样，它的参数初始权重$\rho$使用训练好了的SL Policy的参数权重$\sigma$。</p><p>对弈过程中，用几回合之前的RL Policy Net和最新RL Policy Net对弈。首先随机行成一个对局的开局，然后从该时刻$t=t_0$开始博弈直到游戏结束$t=t_T$。设置胜方$reward(t_T) = 1$，败方$reward(t_T) = -1$，然后对胜负棋局分别用最大似然更新每一步：$$\sum_{s_t} logp_\sigma (a_t|s_t)reward(t)$$</p><h3 id="2-4-价值网络-Value-Network"><a href="#2-4-价值网络-Value-Network" class="headerlink" title="2.4 价值网络(Value Network)"></a>2.4 价值网络(Value Network)</h3><p>Value Network$V^p(s)$用来给当前局面”打分“，即所谓的大局观。Value Network的结构跟RL Policy Net基本一样，只是最后一层的输出由概率分布变成了单个值。<br>Value Network基于以下步骤自我博弈生成了3000万个对局信息：</p><ol><li>随机生成一个数字 $U \sim N(1, 450)$;</li><li>用SL policy network $a_t \sim p_\sigma(·|s_t)$生成前U−1步局面; </li><li>第U步随机选择$a_u ~ unif{1, 361}$，但必须符合围棋规则;</li><li>从第U+1步开始，使用RL Policy Net$a_t \sim p_\rho(·|s_t)$博弈直到游戏结束，得到游戏结果$reward(t_T)$；</li><li>将$(S_{U+1},reward_{U+1})$加入样本池中，用于后续Value Network训练的训练。</li></ol><p>注意：第1、2、3步增加了样本的多样性；损失函数是MSE。<br><img src="/images/pn&vn.png" width="300" alt="ac algorithm" align="center"></p><h2 id="3-在线博弈"><a href="#3-在线博弈" class="headerlink" title="3. 在线博弈"></a>3. 在线博弈</h2><p>在线博弈主要将MCTS与训练好的网络相结合，基于当前局面做出最优的动作选择。</p><h3 id="3-1-蒙特卡洛书搜索-MCTS"><a href="#3-1-蒙特卡洛书搜索-MCTS" class="headerlink" title="3.1 蒙特卡洛书搜索(MCTS)"></a>3.1 蒙特卡洛书搜索(MCTS)</h3><p>MCTS是一种用于博弈游戏寻找最优解决方案的算法。MCTS会基于当前局面做多次模拟博弈，并尝试根据模拟结果预测最优的移动方案。AlphaGo中的MCTS进行一次模拟有以下4步：</p><ol><li><strong>Selection</strong>：在当前局面，根据UCB1算法选择回报值最高的动作：$$a_t = argmax(Q(s_t,a)+\mu(s_t,a)),\quad \mu \propto \frac{P(s,a)}{1+N(s,a)}$$重复选择L步到达一个叶子节点$s_L$。</li><li><strong>Expansion</strong>：利用SL Policy Network，$p_\sigma$的将叶节点$s_L$扩展，并将输出的概率分布作为先验概率$P(s,a)=p_{\sigma}(a|s)$存下来。</li><li><strong>Evaluation</strong>：评估叶节点$s_L$。采用价值网络进行估值$v_\theta(s_L)$，并采用Rollout Policy $p_\pi$博弈直到游戏结束，获得回报$z_L$。</li><li><strong>Backup</strong>：更新蒙特卡洛树。$$\begin{equation}\begin{split}<br>V(s_L) &amp;= (1 - \lambda)v_\theta(s_L) + \lambda z_L\\<br>N(s,a) &amp;= \sum^n_{i=1}1(s,a,i)\\<br>Q(s,a) &amp;= \frac{1}{N(s,a)}\sum^n_{i=1}1(s,a,i)V(s^i_L)<br>\end{split}\end{equation}$$其中$s^i_L$是第i次模拟的叶子节点；$1(s,a,i)$表示(s,a)在第i次模拟是否被访问。当模拟结束，算法会选择访问最多次的动作作为预测动作。</li></ol><p>下图是AlphaGo Nature论文中对MCTS的总结：<br><img src="/images/MCTS.png" width="650" alt="ac algorithm" align="center"></p><h3 id="4-结果模拟"><a href="#4-结果模拟" class="headerlink" title="4. 结果模拟"></a>4. 结果模拟</h3><p><img src="/images/abcdef.png" width="650" alt="ac algorithm" align="center"></p><p>a. 基于当前局面，value network $v_\theta(s’)$对所有的候选位置的评估。<br>b. 仅用value network$v_\theta(s’)$做评估，多次模拟后得到平均Q(s,a)。(即$\lambda = 0$)<br>c. 仅用rollouts network$v_\pi(s,a)$做评估，多次模拟后得到平均Q(s,a)。(即$\lambda = 1$)。<br>d. 根据SL policy network $P_\sigma(a|s)$选择的动作概率（大于0.1%）</p>]]></content>
      
      
      
        <tags>
            
            <tag> Alphago </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>策略梯度</title>
      <link href="/2018/06/17/pg/"/>
      <url>/2018/06/17/pg/</url>
      
        <content type="html"><![CDATA[<p><em>本文涉及算法的实现请参照我的<a href="https://github.com/cheersyouran/reinforcement-learning" target="_blank" rel="noopener">Github</a></em></p><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p>按照<strong>有无模型</strong>分类，强化学习可以分为Model-Based、Model-Free两类，其中Model-Free又可以分为Prediction、Control两类。<br>按照<strong>优化目标</strong>来分类，强化学习可以分为Value-Based、Policy-Based、Actor-Critic三类。</p><p>本文主要说一下Policy-Based方法中的<strong>Stochastic Policy</strong>、<strong>Deterministic policy</strong>以及<strong>Actor-Critic</strong>方法。</p><h2 id="2-Policy-Based方法"><a href="#2-Policy-Based方法" class="headerlink" title="2. Policy-Based方法"></a>2. Policy-Based方法</h2><p>Value-Based方法是通过DP、TD或MC来更新$Q(s,a)$值，并基于$Q(s,a)$值采用 $greedy$或$\epsilon-greedy$策略选择下一个动作。所以我们的策略是根据$Q(s,a)$值<strong>间接</strong>得到的。</p><p>Policy-Based方法不需要学习$Q(s,a)$值，而是根据DP、TD或MC的回报值$R_t$去直接更新策略。所以我们策略是根据输入的状态s<strong>直接</strong>得到的。</p><p>Policy Based方法具体分为两种：Stochastic policy 和 Deterministic policy。区别是前者输出的是动作的概率，后者输出的是一个确定的动作。Policy-Based和Value-Based的优缺点如下：</p><table><thead><tr><th>方法</th><th>适用场景</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>Value-Based</td><td>离散有限的动作空间</td><td>可以每步更新，算法效率较高；可以收敛到全局最优</td><td>无法处理高维连续动作空间</td></tr><tr><td>Policy-Based</td><td>高维连续的动作空间</td><td>更好的收敛性；随机策略自带探索性质</td><td>每个episode更新一次，效率较低，方差较大；容易收敛到局部最优；</td></tr></tbody></table><h3 id="2-1-随机策略-Stochastic-Policy"><a href="#2-1-随机策略-Stochastic-Policy" class="headerlink" title="2.1 随机策略(Stochastic Policy)"></a>2.1 随机策略(Stochastic Policy)</h3><p>随机策略常用如下公式表示：$$\pi_\theta(s,a) = P[a|s,\theta]$$其意义是：在状态s时，动作a符合参数为$\theta$的概率分布。我们的目标是找到最佳策略$\pi_\theta(s,a)$对应的参数$\theta$，因此我们必须有效衡量一个策略的好坏。我们把这个衡量方式称为Performance Objective $J(\theta)$，在神经网络中又称损失函数（目标函数）。常用的$J(\theta)$有如下三种：</p><table><thead><tr><th>名称</th><th>公式</th></tr></thead><tbody><tr><td>start value</td><td>$J_1(\theta)=V^{\pi_\theta}(s_1)=E_{\pi_\theta}[V_1]$</td></tr><tr><td>average value</td><td>$J_{avV}(\theta)=\sum_sd^{\pi_\theta}(s)V^{\pi_\theta}(s)$</td></tr><tr><td>average reward per time-step</td><td>$J_{avR}(\theta)=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)R_s^a$</td></tr></tbody></table><p>有了$J(\theta)$，我们就可以设计一个神经网络来最大化$J(\theta)$（或最小化$-J(\theta)$）。总结来说，在这个神经网络中，输入是当前agent获得的状态s，输出是策略$\pi_\theta(s,a)$，目标是最大化损失函数$J(\theta)$，参数$\theta$的更新方式是梯度上升。</p><p>那随机策略$\pi_\theta(s,a)$究竟是什么呢，又如何表示呢？在介绍策略之前，先介绍一下策略的梯度：<br>$$<br>\begin{equation}\begin{split}<br>\nabla_\theta\pi_\theta(s,a)&amp;=\pi_\theta(s,a)\frac{\nabla_\theta\pi_\theta(s,a)}{\pi_\theta(s,a)}\\<br>&amp;=\pi_\theta(s,a)\nabla_\theta log\pi_\theta(s,a)<br>\end{split}\end{equation}<br>$$推导很简单，其中$\nabla_\theta log\pi_\theta(s,a)$又被称为score function。我们采用$J_{avR}(\theta)$，则$\nabla_ \theta J(\theta)$为:$$\begin{equation}\begin{split}<br>\nabla_ \theta J(\theta)&amp;=\sum_sd^{\pi_\theta}(s)\sum_a\pi_\theta(s,a)\nabla_\theta log\pi_\theta(s,a)R_s^a\\<br>&amp;=E_{s \sim d^\pi, a \sim \pi_\theta}[\nabla_\theta log\pi_\theta(s,a)r]\\<br>&amp;=E_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)r]<br>\end{split}\end{equation}<br>$$通过公式我们可以发现，$\nabla_ \theta J(\theta)$只跟score function和reward有关。<br>回到策略，常用的随机策略有以下两种：</p><table><thead><tr><th>名称</th><th>公式</th><th>score function</th><th>性质</th></tr></thead><tbody><tr><td>Softmax策略</td><td>$\pi_\theta(s,a) \propto e^{\phi(s,a)^T\theta}$</td><td>$\phi(s,a)-E_{\pi_\theta}[\phi(s,·)]$</td><td>适用于离散动作空间</td></tr><tr><td>Gaussian策略</td><td>$a \sim N(\mu,\sigma^2), \mu=\phi(s)^T\theta$</td><td>$\frac{(a-\mu)\phi(s)}{\sigma^2}$</td><td>适用于连续动作空间</td></tr></tbody></table><p>Softmax策略需要将动作离散化，然后按照softmax之后的Q(s,a)值来随机选择动作。Gaussian策略最终会生成一个高斯分布函数，然后按照该分布随机选择动作。如果使用神经网络实现，Softmax策略只需要在网络的最后一层加入一个softmax函数，然后按照softmax之后的概率值选择动作；Gaussian策略一般默认$\sigma$是一个常量，然后用网络来回归$\mu=\phi(s)^T\theta$中的参数$\theta$，然后基于正态分布$N(\mu,\sigma^2)$来随机动作。</p><p>到这里，随机策略梯度的原理基本清晰了。但有几点值得注意：</p><ol><li>Policy based方法中，我们只能得到策略梯度$\nabla J(\theta)$的公式，却并不存在关于一个损失函数的公式。<strong>策略梯度定理</strong>通过严格的证明(参见<a href="https://www.kth.se/social/files/58d506eff27654042836ace7/AllPolicyGradientLecture.pdf" target="_blank" rel="noopener">证明</a>)，得到了与[环境状态转移(transition probability)]无关的$\nabla J(\theta)$。网上普遍说的损失函数$Loss = log\pi_\theta(s,a) r$只是根据$\nabla J(\theta)$反推出来的或根据交叉熵理论推演出来，尚未找到理论证明。在tensorflow中，可以不用tf.minimize而是直接用tf.compute_gradient和tf.apply_gradient来跳过损失函数的问题，当然，使用这种交叉熵$\nabla J(\theta)$也可以同样的结果。</li><li>$Loss = log\pi_\theta(s,a) r$函数中的r需要精心设计。应设计避免 r恒&gt;0或 r恒&lt;0，否者容易导致更新方向固定（恒增或恒减），导致过快收敛于局部最优。理想的r是$E[r]=0$，可参见<a href="https://github.com/cheersyouran/reinforcement-learning" target="_blank" rel="noopener">Github</a>中的例子。若无法避免，应降低学习率，防止过快收敛于局部最优。这也符合Advantages函数的思想。</li><li><p>policy based方法必须等episode结束才能获得reward，才能更新。这种每回合更新一次的算法叫Monte-Carlo Policy Gradient，又叫REINFORCE。参数的更新公式如下：$$\Delta\theta_t=\alpha\nabla_\theta log\pi_\theta(s_t, a_t)v_t$$$$v_t=R_{t+1} + \gamma R_{t+2} + …+\gamma^{T-1}R_{t+2}$$完整的REINFORCE算法如下：<img src="/images/reinforce.png" width="500" alt="REINFROCE algorithm" align="center"></p></li><li><p>严格的来讲，上述PG算法属于on-policy PG，即目标策略和采样策略是相同的策略。off-policy PG公式如下：$$\begin{equation}\begin{split}<br>\nabla_\theta J_\beta(\pi_\theta) &amp;= E_{s \sim \rho^\beta, a \sim \pi_\theta}[\nabla_\theta log \pi_\theta(s,a) Q^\pi(s,a)]\\<br>&amp; = E_{s \sim \rho^\beta, a \sim \beta}[\frac{\pi_\theta(s,a)}{\beta_\theta (s,a)} \nabla_\theta log \pi_\theta(s,a) Q^\pi(s,a)]<br>\end{split}\end{equation}$$其中，$\frac{\pi_\theta(s,a)}{\beta_\theta (s,a)}$是重要性采样。</p></li></ol><h3 id="2-2-Actor-Critic算法"><a href="#2-2-Actor-Critic算法" class="headerlink" title="2.2 Actor-Critic算法"></a>2.2 Actor-Critic算法</h3><h3 id="2-2-1-QAC"><a href="#2-2-1-QAC" class="headerlink" title="2.2.1 QAC"></a>2.2.1 QAC</h3><p>在2.1所描述的REINFROCE算法中，$v_t$虽然是$Q^{\pi_\theta}(s,a)$的抽样无偏估计(unbiased)，但是由于MC抽样的随机性，具有较高的方差(high variance)。如果有可以用一个函数来估计价值函数 $Q_w(s,a) \approx Q^{\pi_\theta}(s,a)$，此时$$\nabla_ \theta J(\theta) \approx E_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_w(s,a)]$$那么不仅可以减少方差，还可以达到<strong>每步更新</strong>，大大提高了算法的效率。这个算法的确有，叫Actor-Critic。</p><p>Actor-Critic算法由Actor和Critic两个模块构成：</p><table><thead><tr><th>模块名称</th><th>作用</th><th>更新方式</th><th>更新公式</th></tr></thead><tbody><tr><td>Actor</td><td>选择动作</td><td>update $\theta$ by Policy Gradient Ascent</td><td>$\theta=\theta+\alpha\nabla_\theta log\pi_\theta(s_t, a_t)Q_w(s,a)$</td></tr><tr><td>Critic</td><td>估计$Q^{\pi_\theta}(s,a)$</td><td>update $w$ by TD(0)</td><td>$w = w + \beta(r + \gamma Q_w(s’,a’) - Q_w(s,a))\nabla_\theta Q_w(s,a)$</td></tr></tbody></table><p>完整的Actor-Critic算法如下：<br><img src="/images/ac.png" width="500" alt="ac algorithm" align="center"></p><p>注意：QAC算法中，仅使用一个线性价值函数$Q_w(s,a)=\phi(s,a)^Tw$来逼近状态行为价值函数$Q^{\pi_\theta}(s,a)$，而没用非线性的神经网络。</p><h3 id="2-2-2-Compatible-Function-Approximation"><a href="#2-2-2-Compatible-Function-Approximation" class="headerlink" title="2.2.2 Compatible Function Approximation"></a>2.2.2 Compatible Function Approximation</h3><p>在2.2.1中，我们得到了$Q^{\pi_\theta}(s,a)$的近似表示$Q_w(s,a)=\phi(s,a)^Tw$，但很遗憾，$Q_w(s,a)$始终是有偏估计，一个有偏的Q值下得到的策略梯度不一是最好的。比如近似值函数$Q_w(s,a)$可能会引起状态重名等。<br>当$Q_w(s,a)$满足如下两个条件时:</p><ol><li>$ \nabla_w Q_w(s,a)=\nabla_\theta log\pi_\theta(s,a)$</li><li>参数使误差$\varepsilon$最小化：$ \varepsilon = E_{\pi_\theta}[(Q^{\pi_\theta}(s,a) - Q_w(s,a))^2]$</li></ol><p>那么我们就可以得到：$$Q_w(s,a) = Q^{\pi_\theta}(s,a)$$$$\nabla_ \theta J(\theta) = E_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)Q_w(s,a)]$$过程很容易证明，只需要令(2)式$\nabla_w \varepsilon$=0即可。略。</p><h3 id="2-2-3-Advantages-Actor-Critic"><a href="#2-2-3-Advantages-Actor-Critic" class="headerlink" title="2.2.3 Advantages Actor-Critic"></a>2.2.3 Advantages Actor-Critic</h3><p>在2.2.2中，我们得到了$Q^{\pi_\theta}(s,a) = Q_w(s,a)$的条件。再此基础上，我们还可以继续改进AC算法。考虑到不同的$Q^{\pi\theta}(s,a)$差距非常大，如上一次100下一次-100等，这样会造成很大的方差。<br>引入Baseline $B(s)$可以一定程度上消除方差。$B(s)$是一个仅与状态值函数$V(s)$有关，跟动作值函数$Q(s,a)$无关的函数。并有如下性质：$$\begin{equation}\begin{split}<br>E_{\pi_\theta}[\nabla_\theta log_{\pi\theta}(s,a)B(s)]&amp;=\sum_s d^{\pi_\theta}(s) \sum_a \nabla_\theta \pi_\theta(s,a)B(s)\\<br>&amp;=\sum_s d^{\pi_\theta}(s)B(s) \nabla_\theta \sum_a \pi_\theta(s,a)\\<br>&amp;=0<br>\end{split}\end{equation}$$故可知：$$E_{\pi_\theta}[\nabla_\theta log_{\pi\theta}(s,a)Q^{\pi_\theta}(s,a)]=<br>E_{\pi_\theta}[\nabla_\theta log_{\pi\theta}(s,a)(Q^{\pi_\theta}(s,a) \pm B(s))]$$</p><p>B(s)可以有很多种选择方式，比较常见的是选择$B(s) = V^{\pi_\theta}(s)$，此时可以定义adavantages function $A^{\pi_\theta}(s,a)$为：$$A^{\pi_\theta}(s,a)=Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s)$$同时$\nabla_ \theta J(\theta)$重新表示为：$$\nabla_ \theta J(\theta) = E_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)A^{\pi_\theta}(s,a)]<br>$$势函数$A^{\pi_\theta}(s,a)$的含义：在s状态采取行为a时，结果相对于状态s的平均值的改善程度。由于减去了状态s的平均估值，故而有效的减小了$Q^{\pi\theta}(s,a)$的方差，使得训练过程更平稳。</p><p>现在我们有了2个价值模型$V^{\pi_\theta}(s)$和$Q^{\pi_\theta}(s,a)$，故需要2个函数$V_v(s)和Q_w(s,a)$去逼近。<br>$$<br>\begin{equation}\begin{split}<br>V_v(s)&amp;\approx V^{\pi_\theta}(s)\\<br>Q_w(s,a) &amp;\approx Q^{\pi_\theta}(s,a)\\<br>A(s,a) &amp;= Q_w(s,a) - V_v(s)\\<br>\end{split}\end{equation}<br>$$</p><p>现在，我们用两个函数已经可以表示$A^{\pi_\theta}(s,a)$，但这样做显然很复杂。有一种很好地解决方法：<br>$$\begin{equation}\begin{split}<br>\delta^{\pi_\theta}&amp;=r+\gamma V^{\pi_\theta}(s’) - V^{\pi_\theta}(s)\\<br>E_{\pi_\theta}[\delta^{\pi_\theta}|s,a] &amp;= E_{\pi_\theta}[r+\gamma V^{\pi_\theta}(s’)|(s,a)] - V^{\pi_\theta}(s)\\<br>&amp;=Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s)\\<br>&amp;=A^{\pi_\theta}(s,a)\\<br>所以，\nabla_ \theta J(\theta) &amp;= E_{\pi_\theta}[\nabla_\theta log\pi_\theta(s,a)\delta^{\pi_\theta}]<br>\end{split}\end{equation}<br>$$实际中，我们用TD的函数逼近形式：$$\delta_v=r+\gamma V_v(s’) - V_v(s)$$这样就只需要更新一个函数的参数v。<br>现在，我们重新整理一下Actor-Critic方法：</p><table><thead><tr><th>模型</th><th>MC</th><th>TD(0)</th><th>TD($\lambda$)</th></tr></thead><tbody><tr><td>Actor</td><td>$\Delta \theta = \alpha(v_t - V_v(s_t)) · $ <br>$ \nabla_\theta log \pi_\theta(s_t,a_t)$</td><td>$\Delta \theta = \alpha(r + \gamma V_v(s_{t+1}) - V_v(s_t)) · $ <br>$ \nabla_\theta log \pi_\theta(s_t,a_t)$</td><td>$\Delta \theta = \alpha(v^\lambda_t - V_v(s_t)) · $ <br>$ \nabla_\theta log \pi_\theta(s_t,a_t)$</td></tr><tr><td>Critic</td><td>$\Delta \theta = \alpha(v_t - V_\theta(s))\phi(s)$</td><td>$\Delta \theta = \alpha(r + \gamma V(s’) - V_\theta(s))\phi(s)$</td><td>$\Delta \theta = \alpha(v^\lambda_t - V_\theta(s))\phi(s)$</td></tr></tbody></table><p>随机策略梯度的多种形式的总结：<img src="/images/pg-summary.png" width="500" alt="REINFROCE algorithm" align="center"></p><h3 id="2-3-确定性策略-Deterministic-policy"><a href="#2-3-确定性策略-Deterministic-policy" class="headerlink" title="2.3 确定性策略(Deterministic policy)"></a>2.3 确定性策略(Deterministic policy)</h3><p>之前说的Stochastic Policy、Actor-Critic都是基于随机策略的。确定性策略跟随机策略不同，确定性策略在同一个状态s的动作a是唯一确定的。确定性策略的公式为：$$a = \mu_\theta(s)$$简单比较一下确定性策略和随机策略：</p><table><thead><tr><th>策略</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>确定性策略</td><td>$\nabla_\theta J(\mu_\theta)$只对状态积分，故需要采样的数据少，效率高</td><td>容易局部最优</td></tr><tr><td>随机策略</td><td>将探索和改进集成到一个策略中</td><td>$\nabla_\theta J(\pi_\theta)$对状态和动作一起积分，算法效率低，需要大量训练数据</td></tr></tbody></table><p>确定性策略的Performance Objective：$$\begin{equation}\begin{split}<br>J(\mu_\theta) &amp;= \int_S \rho^u(s)r(s,u_\theta(s))ds\\<br>&amp;=E_{s \sim \rho^\mu}[r(s,u_\theta(s))]\\<br>&amp;=E_{s \sim \rho^\mu, a = u_\theta(s)}[r(s,a)]\\<br>&amp;=E_{\mu_\theta}[r(s,a)]\\<br>\end{split}\end{equation}$$确定性策略梯度：<br>$$\begin{equation}\begin{split}<br>\nabla_\theta J(\mu_\theta) =\int_S \rho^u(s)\nabla_\theta\mu_\theta(s)\nabla_a Q^{\mu}(s,a)|_{a=\mu\theta(s)}ds\\<br>\end{split}\end{equation}<br>$$</p><p>$$=E_{s \sim \rho^\mu, a = u_\theta(s)}[\nabla_\theta\mu_\theta(s)\nabla_a Q^{\mu}(s,a)]$$</p><p>实际上，Silver在论文中已证明，确定性策略是随机策略在方差为0时的一种特殊形式。</p><h3 id="2-4-Determistic-Actor-Critic"><a href="#2-4-Determistic-Actor-Critic" class="headerlink" title="2.4 Determistic Actor-Critic"></a>2.4 Determistic Actor-Critic</h3><p>在2.3节中，我们将随机行为策略（Actor）与价值策略（Critic）结合，引入了Stochastic Actor-Critic算法。同样的，我们也可以将确定性行为策略（Actor）与价值策略（Critic）结合构造一个Determistic Actor-Critic算法，又叫Determistic Policy Gradient算法。</p><p>与QAC算法相似的，在DPG中引入一个近似函数$Q^w(s,a)$来估计$Q^{\mu_\theta}$，$Q^w(s,a) \approx Q^{\mu_\theta}(s,a)$。此时我们得到$$\nabla_\theta J(\mu_\theta) \approx E_{s \sim \rho^\mu, a = u_\theta(s)}[\nabla_\theta\mu_\theta(s)\nabla_a Q^w(s,a)]$$</p><p>DPG算法的更新方法如下：<br>$$\begin{equation}\begin{split}<br>\delta_t &amp;= r_t+ \gamma Q^w(s_{t+1}, \mu_theta(s_{t+1})) - Q^w(s_t, a_t)\\<br>w_{t+1} &amp;= w_t + \alpha_w \delta_t \nabla_w Q^w(s_t, a_t)\\<br>\theta_{t+1} &amp;= \theta_t + \alpha_\theta \nabla_\theta \mu_\theta(s_t) \nabla_\alpha Q^w(s_t, a_t)<br>\end{split}\end{equation}<br>$$</p><p>值得注意的是，DPG又细分为on-policy和off-policy。上述DPG算法是on-policy的，即想要学习的策略和用于采样的策略相同，critic用sarsa算法训练。off-policy中想要学习的策略和用于采样的策略不同，critic用q-learning训练。不同的采样策略会导致状态s服从不同的分布，所以策略梯度公式会稍稍有变化，参数的更新方式不变：$$\nabla_\theta J(\mu_\theta) \approx E_{s \sim \rho^\beta, a = u_\theta(s)}[\nabla_\theta\mu_\theta(s)\nabla_a Q^w(s,a)]$$</p><p>此外，当满足一下CFA条件时，$Q^w(s,a) = Q^{\mu_\theta}(s,a)$:</p><ol><li>$ \nabla_\theta \mu_\theta(s)^Tw = \nabla_a Q^w(s,a)|_{a=\mu\theta(s)}$</li><li>参数使误差$\varepsilon$最小化：$ \varepsilon = E_{\mu_\theta}[(Q^w(s,a)- Q^{\mu_\theta}(s,a))^2]$</li></ol><p>满足上述条件的$Q^w(s,a)$的线性拟合有很多，比如$Q^w(s,a) = A^w(s,a) + V^v(s)$, 其中$A^w(s,a) = \phi(s,a)^Tw = [\nabla_\theta \mu_\theta(s)(a - \mu_\theta(s))]^Tw$（这里我也不知道为什么要设计成$A^w(s,a) + V^v(s)$）。</p><h3 id="2-5-Deep-Determistic-Policy-Gradient"><a href="#2-5-Deep-Determistic-Policy-Gradient" class="headerlink" title="2.5 Deep Determistic Policy Gradient"></a>2.5 Deep Determistic Policy Gradient</h3><p>DDPG是将深度学习神经网络融合进off-policy DPG的策略学习方法。DDPG针对DPG做了如下改进：</p><ol><li>DPG使用线性函数通过线性回归拟合$Q^{\mu_\theta}$，而DDPG使用神经网络拟合$Q^{\mu_\theta}$。</li><li>借鉴DQN的成功经验，采用经验回放（experience replay）的方式做离线训练，打破数据的时序关联性，保证算法的收敛。</li><li>借鉴DQN的成功经验，引入了online和target两种网络。并采用soft的方式从online critic和online actor对target critic和target actor两个网络进行更新，使学习过程更加稳定。</li><li>采用Batch Normalization解决feature量纲不同的问题</li></ol><p>DDPG算法的伪代码：<img src="/images/ddpg.png" width="550" alt="ddpg algorithm" align="center"></p><p><em>本文涉及算法的实现请参照我的<a href="https://github.com/cheersyouran/reinforcement-learning" target="_blank" rel="noopener">Github</a></em></p><h2 id="3-参考"><a href="#3-参考" class="headerlink" title="3. 参考"></a>3. 参考</h2><p>[1] David Silver. et al. 2014. Deterministic policy gradient algorithms. ICML’14, Vol. 32. JMLR.org I-387-I-395.<br>[2] Richard S. Sutton. et al. 1999. Policy gradient methods for reinforcement learning with function approximation. NIPS’99, MIT Press, Cambridge, MA, USA, 1057-1063.<br>[3] Lillicrap, Timothy P. et al. “Continuous control with deep reinforcement learning.” CoRR abs/1509.02971 (2015): n. pag.</p>]]></content>
      
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 策略梯度 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
